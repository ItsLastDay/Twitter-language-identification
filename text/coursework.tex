\documentclass[a4paper, 14pt]{article}
\usepackage{a4wide}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage[dvips]{graphicx, color}
\usepackage{epstopdf}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{fancyref}
\usepackage{indentfirst}
\usepackage{extsizes}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{multirow} % улучшенное форматирование таблиц
\usepackage{ulem} % подчеркивания
\usepackage{geometry}
\usepackage{breqn}
\geometry{left=3cm}
\geometry{right=1.5cm}
\geometry{top=2.4cm}
\geometry{bottom=2.4cm}
\DeclareMathOperator*{\argmax}{\arg\!\max}

\begin{document}


\linespread{1.3} % полуторный интервал
\renewcommand{\rmdefault}{ftm} % Times New Roman
\frenchspacing
\thispagestyle{empty}


\begin{center}
\ \vspace{-3cm}

\includegraphics[width=0.5\textwidth]{msu.eps}\\

{\scshape Московский государственный университет имени М.~В.~Ломоносова}\\
Факультет вычислительной математики и кибернетики\\
Кафедра системного программирования

\vfill

{\LARGE Курсовая работа}

\vspace{1cm}

{\Huge\bfseries <<Определение языка сообщений социальной сети Twitter>>} \\

\end{center}

\vspace{1cm}

\begin{flushright}
  \large
  \textit{Выполнил студент 327 группы}\\
  М.~А.~Кольцов

  \vspace{5mm}

  \textit{Научный руководитель}\\
  В.~Д.~Майоров
\end{flushright}

\vfill

\begin{center}
Москва, 2014
\end{center}

\pagebreak
\tableofcontents
\pagebreak

\newtheorem{definition} {Определение}
\newtheorem{option} {Свойство}
\newtheorem{theorem} {Теорема}


\section{Введение}
	        В настоящее время человечество имеет доступ к огромному запасу знаний, накопленному за тысячи лет. Немалая часть этих знаний представляется
	        в виде текстов на различных языках. В связи с этим активно разрабатываются методы, предназначенные для автоматического извлечения
	        и преобразования информации, данной в символьном представлении. 
	        Возникло научное направление <<обработка естественных языков>>. Одной из его фундаментальных задач является определение языка текста. 


	        Стандартным подходом к этой задаче является применение машинного обучения. А именно, если у нас есть база из сотен документов 
	        на нескольких языках, то можно <<предсказать>> язык поступившего на рассмотрение документа, сравнив его с имеющимися. 
	        В общем случае, нужно по имеющимся данным построить так называемую модель, а затем все действия с текстами проводить в терминах 
	        этой модели. 
	        Классическим примером является метод, описанный в 1994 году: каждому документу сопоставим <<профиль документа>> --- упорядоченный по
	        числу встречаний список n-грамм --- последовательностей длины n подряд идущих символов. <<Профиль языка>> --- это совокупность профилей
	        документов, которые имеются на этом языке. Теперь, если нужно для какого-то документа определить язык, то последовательность
	        действий такова:
	        \begin{enumerate}
	        		\item Составляется профиль этого документа
	        		\item Сравнивается с имеющимися профилями языков
	        		\item Тот язык, чей профиль наиболее похож на профиль документа, объявляется результатом
	        \end{enumerate}
	          
	        Вышеописанный метод плохо работает для коротких сообщений. В то же время, поток информации в виде коротких шумных сообщений нельзя игнорировать ---
	        в социальной сети Twitter среднее количество сообщений в день составляет примерно 58 000 000, а длина каждого ограничена 140 символами. 
	        Такой формат текстов обусловил появление новых алгоритмов. В данной работе рассматриваются современные методы 
	        решения задачи определения языка, а также предлагается улучшение для одного из них. Проводится тестирование, показывающее
	        превосходство по полноте распознавания языка полученного результата над существующими.
  
	        
\section{Постановка задачи}
		\subsection{Формальное описание задачи автоматического определения языка}
		Пусть $L$ - множество меток, сопоставленных естественным языкам.
		По заданному тренировочному корпусу $$ T = \{(msg_{1}, lang_{1}), (msg_{2}, lang_{2}), \ldots, (msg_{N}, lang_{N}) \} $$
		(здесь $msg_{i}, i \in \overline{1..N},$ - текст на естественном языке, $lang_{i} \in L$ - метка этого языка) нужно построить классификатор,
		который произвольному входному сообщению $new\_msg$ на языке $some\_lang$ сопоставит метку $l \in L$, соответствующую этому языку,
		 или же сообщит, что язык текста невозможно достоверно распознать.
		\subsection{Цели и задачи курсовой работы}
		В данной работе в качестве текстов выступают так называемые <<твиты>> - сообщения из социальной сети Twitter\footnote{https://twitter.com/}, 
		а множество $L$ соответствует 18 языкам, которые можно разделить на несколько групп по типу алфавита:
		\begin{itemize}
			\item Кириллические: болгарский, чувашский, русский, татарский, украинский
			\item Арабские: арабский, персидский (фарси), урду
			\item Латинские: нидерландский, французский, английский, немецкий, итальянский, испанский, турецкий
			\item Деванагари: хинди, маратхи, непальский
		\end{itemize}
		Цели работы:
		\begin{enumerate}
			\item Исследовать современные решения задачи автоматического определения языка коротких сообщений
			\item Провести совместное сравнительное тестирование некоторых методов решения задачи и выяснить, действительно ли они показывают
			заявленное авторами качество классификации
			\item Исследовать зависимость качества классификации от различных факторов: степени нормализации сообщений, мощности обучающей 
			выборки, возможных предположений о структуре текстов
			\item Исследовать возможность улучшения какого-либо алгоритма решения задачи автоматического определения языка коротких сообщений	
		\end{enumerate}
		В рамках выполнения цели работы необходимо решить следующие задачи:
		\begin{enumerate}
			\item Собрать обучающий корпус, состоящий не менее чем из 800 твитов для каждого языка
			\item Реализовать один или несколько методов решения задачи автоматического определения языка коротких сообщений
			\item Организовать удобный интерфейс для автоматического тестирования методов
			\item Выбрать меру качества классификации
		\end{enumerate}

\section{Обзор существующих решений}
		\subsection{Подход, основанный на частоте n-грамм}
		Для каждого языка составляется список из $K$ самых часто встречающихся n-грамм. Для $new\_msg$ составляется аналогичный список, для него
		определяется наиболее <<похожий>> из списков языков
		и на основании этого делается вывод о языке сообщения. 
		
		<<Похожесть>> двух упорядоченных списков характеризуется метрикой \textit{out-of-place}: 
		пусть в одном списке n-грамма $x$ стоит на позиции $i$, а в другом \nolinebreak --- $j$, тогда к расстоянию между списками добавляется $|i - j|$.
		
		
		Cavnar \& Trenkle показали, что при значении $K \approx 300$, составленные 
		профили языков очень хорошо эти языки характеризуют. Также они предлагают использовать длину n-грамм вплоть до 5.
		
		Этим подходом пользуются многие программные продукты, например, \textit{langid}, \textit{TextCat}, \textit{Google CLD}, о которых речь пойдёт
		в части 4.
		
		\noindent Плюсы и минусы:
		
		
		$+$ Малый размер модели
		
		$+$ Высокая скорость обучения и классификации
		
		$-$ Качество классификации сильно зависит от длины текста
		
		$-$ Много информации из документов не попадает в модель
		
		\subsection{Prediction by partial matching}
		Кодируются все имеющиеся документы, основываясь на частотах встречания цепочек из не более чем $n$ символов (чем больше частота --- тем короче
		код входящих в цепочку символов). Затем для $new\_msg$ вычисляется длина кода при помощи полученного кодирования и выбирается наиболее
		компактно кодирующий язык.
		
		Такой подход предлагается, например, в \cite{ppm}
		
		\noindent Плюсы и минусы:
		
		$+$ 	Мало параметров
		
		$+$ Высокая точность классификации
		
		$-$ Для нормального функционирования нужны корпуса текстов, измеряемые мегабайтами
		
		\subsection{Подходы, использующие <<стандартные>> алгоритмы машинного обучения}
		Из документов вычленяются признаки, которые их характеризуют, и каждому документу сопоставляется вектор признаков. Далее применяется
		один из алгоритмов машинного обучения (например, логистическая регрессия или метод опорных векторов) для построения модели зависимости
		целевой переменной (языка сообщения) от вектора признаков. $New\_msg$ при классификации точно также заменяется на вектор признаков,
		а затем классификация проходит согласно выбранному алгоритму.
		
		Благодаря гибкости в выборе признаков, подход является перспективным по отношению к определению языка твитов. Так, например, в работах \cite{ppm} \cite{lrev}
		используются, помимо текстовых, такие признаки: имя пользователя в Twitter; его местоположение; язык интерфейса; наиболее вероятный
		язык в предыдущих $k$ твитах этого пользователя; наиболее вероятный язык сущностей, на который ссылается пользователь в сообщении: текстов
		других пользователей, ссылок, тэгов.
		
		\noindent Плюсы и минусы:
		
		$+$ Хорошо разработанный набор стандартных алгоритмов
		
		$+$ Возможность настройки для выбранной предметной области
		
		$-$ Нужно тщательно подбирать признаки и алгоритм, дабы получить адекватный результат
		
		\subsection{Language identification graph-based approach (LIGA)}
		По каждому документу обучающей выборки строится граф $(V, E)$:
		 каждой вершине $v \in V$ графа соответствует пара $(trigram, count)$, где $trigram$ - это одна
		из триграмм, встретившихся в тексте, а $count$ - количество её вхождений. Будем обозначать за $v_{tr}$ соответствующую вершине $v$ триграмму
		($v_{tr} \neq u_{tr} \ \forall u \neq v$), за $v_{cnt}$ - количество её вхождений. 
		Ориентированное ребро $(u, v) \in E$ соответствует тому, что в тексте $u_{tr}$ следует непосредственно
		перед $v_{tr}$, а $(u, v)_{cnt}$ - количество раз, когда такое происходило.
		
		Далее, объединим полученные графы для всех документов одного языка $l \in L$: если у двух графов $(V', E')$ и $(V'', E'')$ есть вершины
		$v' \in V'$ и $v'' \in V''$ такие, что $v'_{tr} = v''_{tr}$, то в результирующем графе $(V_{l}, E_{l})$ должна быть вершина 
		$v \in V_{l}$ такая, что 
		$$v_{tr} = v'_{tr} = v''_{tr}$$ $$v_{cnt} = v'_{cnt} + v''_{cnt}$$
		Аналогично выполняется объединение рёбер.
		
		Последний шаг в построении модели: объединить графы $(V_{l}, E_{l})$, полученные $\forall l \in L$, в один. 
		Для этого каждой вершине $v \in V_{l}$
		(ребру $(u, v) \in E_{l}$) ставится дополнительно в соответствие метка языка $v_{lang}$ \ ($(u, v)_{lang}$). 
		Готовая модель представляет собой граф
		($\mathcal{V}, \mathcal{E}$), который получается следующим образом:
		$$ \mathcal{V} = \bigcup_{l \in L} V_{l}$$
		$$ \mathcal{E} = \bigcup_{l \in L} E_{l}$$
		
		Классификация происходит разбиением на триграммы, а затем <<укладыванием>> этих триграмм на пути графа. Формально, если
		$t_{1}, t_{2}, \ldots, t_{n}$ - триграммы сообщения $new\_msg$ в порядке их вхождения, то результатом будет
		$$ \argmax_{l} score_{l} ,$$
		где вектор \textit{score} длины $|L|$ определяется как:
		$$ score_{l} = \sum_{i=1}^{n} get\_v(t_{i}, l) + \sum_{i=1}^{N-1} get\_e(t_{i}, t_{i+1}, l)$$
		\[
 		get\_v(tr, l) =
 		  \begin{dcases}
  		   v_{cnt} \over \sum_{w \in \mathcal{V}} w_{cnt} & 
  		   \begin{split} 
					 \exists v \in \mathcal{V}: v_{tr}=tr \\ v_{lang}=l	
  		   	\end{split} \\
  		   0 & \quad otherwise
  		 \end{dcases}
		\]		
		
		\[
 		get\_e(tr_1, tr_2, l) =
 		  \begin{dcases}
  		   (u, v)_{cnt} \over \sum_{(w, q) \in \mathcal{E}} (w, q)_{cnt} & 
  		    \begin{split} 
					 \exists (u, v) \in \mathcal{E}:  u_{tr}=tr_1 \\ v_{tr}=tr_2 \\ (u, v)_{lang}=l
  		   	\end{split} \\
  		   0 & \quad otherwise
  		 \end{dcases}
		\]	
		
		Метод предложен в работе \cite{liga}. Такая модель позволяет зафиксировать одновременно частотность триграмм и их положение, при этом все триграммы
		из документов участвуют в классификации. Например, неправильное написание какого-то слова, будучи упущенным в n-граммном подходе (поскольку он
		учитывает лишь наиболее часто встречающиеся сочетания), добавит определённости при определении языка методом LIGA.		
		
		\noindent Плюсы и минусы:
		
		$+$ Высокая скорость обучения и классификации
		
		$+$ Максимально использует информацию из обучающей выборки
		
		$+$ Возможность дообучения <<на лету>>
		
		$+$ Возможность визуализации модели
		
		$-$ Большой объём модели и, как следствие, относительно высокие требования к памяти
			
\section{Подготовка к тестированию}
		\subsection{Сравниваемые системы}
			\subsubsection{TextCat}
			TextCat\footnote{http://odur.let.rug.nl/vannoord/TextCat/} --- авторская реализация подхода Canvar \& Trenkle \cite{canvar}, основанного на n-граммах.
			Этот классический продукт часто берётся в качестве стандарта при сравнении алгоритмов определения языка. 
			Оригинальные модели языков достаточно устарели, но, к счастью, у программы есть возможность обучения.
			\subsubsection{Google CLD}
			Google compact language detector\footnote{https://code.google.com/p/cld2/} --- программный продукт, встроенный в браузер Chromium, 
			предназначенный для определения языка просматриваемой страницы. 
			Использует 4-граммы и умеет считывать метаинформацию
			о веб-страницах. Впрочем, для коротких текстов он тоже используется, например, в сервисе Google Translate. Система поставляется обученной
			на огромном корпусе текстов на более чем 100 языках.
			\subsubsection{Langid.py}
			Программа langid.py\footnote{https://github.com/saffsd/langid.py} за авторством Lui \& Baldwin \cite{langid}, выпущенная в 2011 году, позиционируется
			как быстрое и точное решение задачи определения языка текста. Продукт поддерживает 97 языков.
			\subsubsection{LIGA}
			Реализация метода LIGA, выполненная в рамках курсовой работы в соответствии со статьёй \cite{liga}. Подход подробно описан в секции 3.4.
			\subsubsection{LIGAv2}
			Я предлагаю улучшенную версию алгоритма LIGA. Основное отличие заключается в изменении функций get\_v и get\_e:
			\[
 			get\_v\_v2(tr, l) =
 			  \begin{dcases}
  			   {(\log{|L| \over |\{r \in L | \exists v \in \mathcal{V}: v_{tr}=tr, v_{lang}=r\}|} + 1) \times  v_{cnt}} \over 
  			   \sum_{w \in \mathcal{V},\ w_{lang}=l} w_{cnt} & 
  			   \begin{split} 
						 \exists v \in \mathcal{V}: v_{tr}=tr \\ v_{lang}=l	
  			   	\end{split} \\
  			   0 & \quad otherwise
  			 \end{dcases}
			\]		
		
			\[
 			get\_e\_v2(tr_1, tr_2, l) =
 			  \begin{dcases}
  			   {(\log{|L| \over edges} + 1) \times (u, v)_{cnt}} 
  			   \over \sum_{(w, q) \in \mathcal{E},\ (w, q)_{lang}=l} (w, q)_{cnt} & 
  			    \begin{split} 
						 \exists (u, v) \in \mathcal{E}:  u_{tr}=tr_1 \\ v_{tr}=tr_2 \\ (u, v)_{lang}=l
  			   	\end{split} \\
  			   0 & \quad otherwise
  			 \end{dcases}
			\]	
			$$ edges = |\{r \in L \ | \ \exists (u, v) \in \mathcal{E}: u_{tr}=tr_1, v_{tr}=tr_2, (u, v)_{lang}=r\}| $$
			Мотивация к такому изменению следующая. Гораздо полезнее знать, насколько специфична данная триграмма данному языку, чем то, какой процент
			раз она встречалась в обучающей выборке вообще. Для этого в знаменателе общая сумма очков всех вершин графа (аналогично для рёбр) заменена 
			на сумму очков всех вершин подграфа $(V_{l}, E_{l})$. Также вводится логарифмический коэффициент, идея которого 
			навеяна idf\footnote{inverse document frequency}, и который характеризует <<уникальность>> данной триграммы в обучающей выборке.
			
			После таких изменений появляется шанс, что фраза \textit{"Привiт, груша"} будет корректно распознана как украинская, поскольку триграммы
			\textit{вiт} и \textit{ивi} будут иметь больший вес, чем остальные.
			\subsubsection{LogR}
			Реализация метода LogR, выполненная в рамках курсовой работы в соответствии со статьёй \cite{ppm}. Общий подход описан в секции 3.3. Данная версия использует логистическую регрессию из пакета LibLinear \cite{fan}, а также следующие признаки:
			\begin{itemize}
				\item Логарифм частоты встречания каждой уни-, би-, три- и квадграммы.
				\item Имя оставившего твит пользователя Twitter, его местоположение и отображаемое имя, а также их префиксы
				\item Индикатор: написано ли имя пользователя латиницей
				\item Встреченные в твите хэштеги
					\footnote{Хэштег - это последовательность символов без пробелов, начинающаяся с '$\#$', например: \textit{\#HarryPotter}, \textit{\#russia2014}}
				\item Встреченные в твите упоминания
					\footnote{Упоминание - это отображаемое имя пользователя, предварённое символом '@', например: \textit{@KremlinRussia}}
				\item Доменное имя 1 и 2 уровней, а также протокол для всех ссылок, встреченных в сообщении (при этом, если ссылка была сокращённой,
				например bit.ly/something, то происходит http-запрос для определения конечной цели)
			\end{itemize}
		\subsection{Обучающая выборка}   
			Твиты для обучающей выборки собирались из следующих мест:
			\begin{enumerate}
				\item \textbf{Приложенные к статьям.} В работе \cite{liga} тексты сообщений оказались в открытом доступе, около 9 тысяч штук на европейских языках. Также авторы статьи \cite{ppm} поделились собранным корпусом, в котором около 6 тысяч твитов, содержащих помимо текста ещё и метаинформацию о пользователях. Огромный набор текстов на русском языке прилагается к статье \cite{julia} - более двухсот тысяч твитов.
				\item \textbf{Извлечённые с помощью TwitterAPI по идентификационному номеру.} Согласно политике Twitter, в открытом доступе могут находиться не тексты сообщений, а лишь их идентификационные номера. Вместе с работой \cite{lrev} распространяется архив, содержащий номера твитов, используемых в ней при тестировании. 
				\item \textbf{С сайта Indigenous Tweets.}\footnote{http://indigenoustweets.com/} Данная веб-страница содержит список малопредставленных в Twitter языков, а также перечисление всех известных пользователей с указанием количества сообщений, которое они оставили на конкретном языке. Были выделены пользователи, пишущие в основном на татарском и чувашском языках, а их сообщения были выкачаны с помощью TwitterAPI. Таким образом было получено 6 тысяч твитов.
				\item \textbf{С помощью собственного аккаунта.} В распоряжении оказался аккаунт турецкого пользователя, подписанного на новостные рассылки на турецком языке. Было извлечено 800 твитов на турецом. Такое малое количество обусловлено ограничениями TwitterAPI.
			\end{enumerate}		
			Итоговое распределение по языкам (после нормализации по схеме 2) показано в таблице 1.	  
			 
			\begin{center}
			\begin{table}
			\begin{tabular*}{\textwidth}{|@{\extracolsep{\fill} }l  r|}
				\hline
				Язык  & Количество твитов \\
				\hline
				Арабский & 1171 \\
				Английский & 2234 \\
				Болгарский & 1880 \\
				Испанский & 2350 \\
				Итальянский & 1538 \\
				Маратхи & 1154 \\
				Немецкий & 2208 \\
				Непальский & 1678 \\
				Нидерландский & 2032 \\
				Персидский (фарси) & 2361 \\
				Русский & 196836 \\
				Татарский & 3248 \\
				Турецкий & 781 \\
				Украинский & 627 \\
				Урду & 1073 \\
				Французский & 2239 \\
				Хинди & 1209 \\
				Чувашский & 2584 \\
				\hline
				Всего & 227203 \\
				\hline
			\end{tabular*}
			\caption{Распределение по языкам твитов обучающей выборки}
			\end{table}
			\end{center}		
			
		\subsection{Описание сценариев нормализации}	
		Сообщения в социальных сетях часто содержат ошибки, выражения эмоций, неправильное употребление слов и знаков препинания. Чтобы классификатор
		был устойчивым к такого рода явлением, нужно \textit{нормализовывать}	 каждое сообщение --- то есть приводить его к какому-то стандартному виду.
		Дабы проследить зависимость качества классификации от степени нормализации текстов, были использованы следующие сценария нормализации:
		\begin{enumerate}
			\item Из твита удаляются отметки о ретвитах\footnote{Например, RT @KremlinRussia}, ссылки, упоминания, хэштеги, символы пунктуации заменены на пробел, цифры удалены, последовательно идущие пробельные символы заменены на один пробел
			\item В дополнение к пункту 1: все последовательности из трёх и более подряд идущих одинаковых символов заменены на два символа (например, слово \textit{оооооооу} превращается таким образом в \textit{ооу}), все слова из 2 и менее букв удалены, текст приведён в нижний регистр.
		\end{enumerate}
		В обоих случаях после нормализации сообщения, в которых не осталось никаких символов, выбрасывались из обучающей выборки.
		
		\subsection{Выбор меры качества классификации}
		В данной работе в качестве меры качества используется \textit{полнота}, то есть доля правильно определённых сообщений на фиксированном языке
		от общего количества твитов на этом языке.
		
		Выбор обусловлен тем, что классификатор LIGA, ввиду большого числа обучающих данных на русском языке,
		выдаёт почти всем кириллическим текстам метку <<русский>>. В то же время, на несколько десятков, например, болгарских сообщений он выдаёт 
		<<болгарский>>. Если посчитать \textit{точность} определения болгарского языка --- отношение числа правильно определённых болгарских твитов 
		к числу твитов, определённых как болгарские --- то она будет близка к 100\%. Ожидаемо, точность распознавание русского языка при этом падает.
		Но, так как русских сообщений на два порядка больше, то падает она примерно на 1\%. Таким образом, высокие показатели \textit{точности} на деле
		скрывают неудовлетворительные результаты классификации.
		
		В то же время, полнота соответствует интуитивному понятию <<точности>>.
       
\section{Результаты тестирования}
	
	\begin{center}
		\begin{tikzpicture}
		\begin{axis}[xlabel=errors, ylabel=percentage]
		\addplot[blue!80!black] coordinates
			{(-23, 1.5) (0, 1.7) (10, 1.96)};
		\addlegendentry{liga}
		\end{axis}
		\end{tikzpicture}
	\end{center}		
	
\section{Заключение}
		В рамках данной работы предполагалось изучить актуальные методы решения задачи автоматического определения языка сообщений социальной сети Twitter, провести сравнительное тестирование некоторых, исследовать возможность улучшения качества классификации. 
		
		Для достижения этих целей были достигнуты следующие результаты:
		\begin{enumerate}
			\item Реализовано 2 подхода к решению этой задачи на языке программирования \textit{Python}, а также модернизированная версия одного из них
			\item Собрана обучающая выборка из более чем двухсот тысяч сообщений
			\item Написан комплекс скриптов для проведения совместного тестирования полученных решений с программными продуктами, успешно используемыми в области обработки естественых языков
		\end{enumerate}
		Также была исследована зависимость качества классификации от степени нормализации сообщений, а также от предположения о структуре текстов ---
		<<твит либо написан на своём языке, либо на своём с примесями английского>>. В обоих случаях наблюдалось ухудшение качества работы.
		
		Тестирование показало, что улучшенная версия метода LIGA не уступает, а зачастую и превосходит по полноте классификации все остальные подходы.
		
		

  \begin{thebibliography}{99}
    \bibitem{canvar} Cavnar W. B. et al. N-gram-based text categorization //Ann Arbor MI. – 1994. – Т. 48113. – №. 2. – С. 161-175.
    \bibitem{liga} Tromp E., Pechenizkiy M. Graph-based n-gram language identification on short texts //Proc. 20th Machine Learning conference of Belgium and The Netherlands. – 2011. – С. 27-34.
    \bibitem{ppm} Bergsma S. et al. Language identification for creating language-specific Twitter collections //Proceedings of the Second Workshop on Language in Social Media. – Association for Computational Linguistics, 2012. – С. 65-74.
    \bibitem{lrev} Carter S., Weerkamp W., Tsagkias M. Microblog language identification: Overcoming the limitations of short, unedited and idiomatic text //Language Resources and Evaluation. – 2013. – Т. 47. – №. 1. – С. 195-215.
    \bibitem{julia} Ю.В. Рубцова. Метод построения и анализа корпуса коротких текстов для задачи классификации отзывов // Электронные библиотеки: перспективные методы и технологии, электронные коллекции: Труды XV Всероссийской научной конференции RCDL’2013, Ярославль, Россия, 14-17 октября 2013 г. – Ярославль: ЯрГУ, 2013. –С. 269-275.
    \bibitem{fan} Fan R. E. et al. LIBLINEAR: A library for large linear classification //The Journal of Machine Learning Research. – 2008. – Т. 9. – С. 1871-1874.
    \bibitem{langid} Baldwin T., Lui M. Language identification: The long and the short of the matter //Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. – Association for Computational Linguistics, 2010. – С. 229-237.
  \end{thebibliography}
\end{document}
